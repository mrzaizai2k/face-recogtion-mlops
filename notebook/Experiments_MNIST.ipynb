{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "sx9e_pXlCuti"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "try:\n",
    "    print(file_path)\n",
    "except:\n",
    "    file_path = os.path.abspath('')\n",
    "    os.chdir(os.path.dirname(file_path))\n",
    "    print(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BcmGBqXeCutw"
   },
   "source": [
    "# Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System libraries\n",
    "import os\n",
    "import torch\n",
    "from torchinfo import summary\n",
    "# from torchvision import transforms as T\n",
    "from src.Utils.utils import *\n",
    "from src.facenet_triplet.utils import *\n",
    "from facenet_pytorch import InceptionResnetV1\n",
    "\n",
    "import torch\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.optim as optim\n",
    "\n",
    "from src.facenet_triplet.trainer import fit\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import InterpolationMode , v2\n",
    "\n",
    "from src.facenet_triplet.metrics import AverageNonzeroTripletsMetric\n",
    "\n",
    "# Set up data loaders\n",
    "from src.facenet_triplet.datasets import TripletFace, BalancedBatchSampler\n",
    "from src.facenet_triplet.networks import TripletNet, FacenetEmbeddingNet\n",
    "from src.facenet_triplet.losses import TripletLoss, OnlineContrastiveLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "facenet_config_path = 'config/facenet.yaml'\n",
    "facenet_config = read_config(path = facenet_config_path)\n",
    "EPOCHS = facenet_config['EPOCHS']\n",
    "PATIENCE = facenet_config['PATIENCE']\n",
    "BATCH_SIZE = facenet_config['BATCH_SIZE']\n",
    "IMG_SIZE = facenet_config['IMG_SIZE']\n",
    "RANDOM_SEED = facenet_config['RANDOM_SEED']\n",
    "WEIGHT_DECAY = facenet_config['WEIGHT_DECAY']\n",
    "LR_WARMUP = facenet_config['LR_WARMUP']\n",
    "CLIP_GRAD_NORM = facenet_config['CLIP_GRAD_NORM']\n",
    "PRETRAINED_MODEL = facenet_config['PRETRAINED_MODEL']\n",
    "MODEL_DIR = facenet_config['MODEL_DIR']\n",
    "PIN_MEMORY = facenet_config['PIN_MEMORY']\n",
    "IMG_SIZE = facenet_config['IMG_SIZE']\n",
    "\n",
    "log_interval = facenet_config['log_interval']\n",
    "learning_rate = facenet_config['learning_rate']\n",
    "margin = facenet_config['margin']\n",
    "\n",
    "\n",
    "MODEL_DIR = rename_model(model_dir = MODEL_DIR, prefix='facenet')\n",
    "facenet_config['MODEL_DIR'] = MODEL_DIR\n",
    "NUM_WORKERS = 0 if os.name == 'nt' else 8\n",
    "\n",
    "\n",
    "train_dir = facenet_config['train_dir']\n",
    "test_dir = facenet_config['test_dir']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_folders(path):\n",
    "    \"\"\" Count the number of folders in the given directory \"\"\"\n",
    "    return len([name for name in os.listdir(path) if os.path.isdir(os.path.join(path, name))])\n",
    "\n",
    "\n",
    "num_classes = count_folders(train_dir)\n",
    "print(f'Number of classes: {num_classes}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_original = v2.Compose([\n",
    "    v2.Resize(IMG_SIZE, interpolation=InterpolationMode.BICUBIC,),\n",
    "    v2.CenterCrop(IMG_SIZE),\n",
    "    v2.ToTensor(),\n",
    "    v2.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "])\n",
    "train_face_dataset = datasets.ImageFolder(train_dir, transform=transform_original)\n",
    "test_face_dataset = datasets.ImageFolder(test_dir, transform=transform_original)\n",
    "train_face_dataset.train =True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_face_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_face_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_model = InceptionResnetV1(pretrained=PRETRAINED_MODEL, classify=False,\n",
    "                               num_classes=None, device=device)\n",
    "facenet_embedding_net = FacenetEmbeddingNet(res_model)\n",
    "facenet_embedding_net = facenet_embedding_net.to(device)\n",
    "facenet_model = TripletNet(facenet_embedding_net)\n",
    "facenet_model = facenet_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplet_train_face_dataset = TripletFace(train_face_dataset, random_seed=RANDOM_SEED) # Returns triplets of images\n",
    "triplet_test_face_dataset = TripletFace(test_face_dataset, random_seed=RANDOM_SEED) # Returns triplets of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_triplet(triplet_train_face_dataset[10][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {'num_workers': NUM_WORKERS, 'pin_memory': PIN_MEMORY} if cuda else {}\n",
    "\n",
    "triplet_train_face_loader = torch.utils.data.DataLoader(triplet_train_face_dataset, batch_size=BATCH_SIZE, shuffle=True, **kwargs)\n",
    "triplet_test_face_loader = torch.utils.data.DataLoader(triplet_test_face_dataset, batch_size=BATCH_SIZE, shuffle=False, **kwargs)\n",
    "\n",
    "\n",
    "loss_fn = TripletLoss(margin)\n",
    "\n",
    "optimizer = optim.Adam(facenet_model.parameters(), lr=learning_rate)\n",
    "scheduler_linear = lr_scheduler.LinearLR(optimizer, start_factor=0.01, total_iters=LR_WARMUP)\n",
    "scheduler_cosine = lr_scheduler.CosineAnnealingLR(optimizer, T_max=490, eta_min=learning_rate/100)\n",
    "scheduler = lr_scheduler.SequentialLR(optimizer, [scheduler_linear,scheduler_cosine],milestones=[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit(train_loader = triplet_train_face_loader, \n",
    "#     val_loader=triplet_test_face_loader, \n",
    "#     model= facenet_model, \n",
    "#     loss_fn=loss_fn,\n",
    "#     optimizer=optimizer, \n",
    "#     scheduler = scheduler, \n",
    "#     n_epochs=EPOCHS, \n",
    "#     device=device, \n",
    "#     log_interval=log_interval,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(facenet_model.state_dict(), \"models/facenet_tune/facenet.pt\")\n",
    "torch.save(facenet_model, MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online Pair Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_face_dataset.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll create mini batches by sampling labels that will be present in the mini batch and number of examples from each class\n",
    "train_batch_sampler = BalancedBatchSampler(train_face_dataset, n_classes=3, n_samples=2, is_dataset=True)\n",
    "test_batch_sampler = BalancedBatchSampler(test_face_dataset, n_classes=3, n_samples=2, is_dataset=True)\n",
    "\n",
    "kwargs = {'num_workers': NUM_WORKERS, 'pin_memory': PIN_MEMORY} if cuda else {}\n",
    "\n",
    "online_train_loader = torch.utils.data.DataLoader(train_face_dataset, batch_sampler=train_batch_sampler, **kwargs)\n",
    "online_test_loader = torch.utils.data.DataLoader(test_face_dataset, batch_sampler=test_batch_sampler, **kwargs)\n",
    "\n",
    "\n",
    "loss_fn = OnlineContrastiveLoss(margin, HardNegativePairSelector())\n",
    "optimizer = optim.Adam(facenet_model.parameters(), lr=learning_rate)\n",
    "scheduler_linear = lr_scheduler.LinearLR(optimizer, start_factor=0.01, total_iters=LR_WARMUP)\n",
    "scheduler_cosine = lr_scheduler.CosineAnnealingLR(optimizer, T_max=490, eta_min=learning_rate/100)\n",
    "scheduler = lr_scheduler.SequentialLR(optimizer, [scheduler_linear,scheduler_cosine],milestones=[10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Assuming BalancedBatchSampler is already defined\n",
    "\n",
    "# Parameters\n",
    "n_classes = 2  # Number of classes per batch\n",
    "n_samples = 3   # Number of samples per class\n",
    "batch_size = n_classes * n_samples\n",
    "\n",
    "# Create the balanced batch sampler\n",
    "# sampler = BalancedBatchSampler(dataset, n_classes=n_classes, n_samples=n_samples)\n",
    "sampler = BalancedBatchSampler(train_face_dataset, n_classes=n_classes, n_samples=n_samples, is_dataset=True)\n",
    "loader = DataLoader(train_face_dataset, batch_sampler=sampler, **kwargs)\n",
    "\n",
    "def check_batch_balance(batch_labels):\n",
    "    label_counts = Counter(batch_labels.numpy())\n",
    "    print(f\"Batch size: {len(batch_labels)}\")\n",
    "    print(f\"Unique classes in batch: {len(label_counts)}\")\n",
    "    print(f\"Samples per class: {label_counts}\")\n",
    "    is_balanced = all(count == n_samples for count in label_counts.values())\n",
    "    return is_balanced\n",
    "\n",
    "# Check a few batches\n",
    "for i, (_, labels) in enumerate(loader):\n",
    "    print(f\"\\nBatch {i + 1}:\")\n",
    "    is_balanced = check_batch_balance(labels)\n",
    "    print(f\"Is batch balanced? {is_balanced}\")\n",
    "    \n",
    "    if i == 4:  # Check 5 batches\n",
    "        break\n",
    "\n",
    "print(\"\\nSampler length:\", len(sampler))\n",
    "print(\"Loader length:\", len(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use facenet_embedding_net not the facenet_model\n",
    "# fit(train_loader = online_train_loader, \n",
    "#                 val_loader=online_test_loader, \n",
    "#                 model= facenet_embedding_net, \n",
    "#                 loss_fn=loss_fn,\n",
    "#                 optimizer=optimizer, \n",
    "#                 scheduler = scheduler, \n",
    "#                 n_epochs=EPOCHS, \n",
    "#                 device=device, \n",
    "#                 log_interval=log_interval,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(facenet_embedding_net, MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# facenet_model = torch.load(\"models/facenet_tune/facenet_2024_07_15_1.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_result(facenet_model, triplet_train_face_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facenet_embedding_net = torch.load(\"models/facenet_tune/facenet_2024_07_16_1.pth\")\n",
    "# facenet_embedding_net = facenet_embedding_net.to(device)\n",
    "facenet_model_2 = TripletNet(facenet_embedding_net)\n",
    "facenet_model_2 = facenet_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_result(facenet_model_2, triplet_train_face_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from facenet_pytorch import MTCNN, InceptionResnetV1, fixed_image_standardization, training\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './data/NAB_faces'\n",
    "output_dir = 'data/NAB_faces_cropped'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device ='cpu'\n",
    "# mtcnn = MTCNN(\n",
    "#     image_size=160, margin=0, min_face_size=20,\n",
    "#     thresholds=[0.6, 0.7, 0.7], factor=0.709, post_process=True, select_largest=True,\n",
    "#     device=device\n",
    "# )\n",
    "\n",
    "# dataset = datasets.ImageFolder(data_dir, transform=transforms.Resize((512,512)))\n",
    "# dataset.samples = [\n",
    "#     (p, p.replace(data_dir, data_dir + '_cropped'))\n",
    "#         for p, _ in dataset.samples\n",
    "# ]\n",
    "        \n",
    "# loader = DataLoader(\n",
    "#     dataset,\n",
    "#     num_workers=0,\n",
    "#     batch_size=8,\n",
    "#     collate_fn=training.collate_pil\n",
    "# )\n",
    "\n",
    "# for i, (x, y) in enumerate(loader):\n",
    "#     try:\n",
    "#         mtcnn(x, save_path=y)\n",
    "#         print('\\rBatch {} of {}'.format(i + 1, len(loader)), end='')\n",
    "#     except:\n",
    "#         continue\n",
    "    \n",
    "# # Remove mtcnn to reduce GPU memory usage\n",
    "# del mtcnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "# Define device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((160, 160)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    filenames = []\n",
    "    for filename in os.listdir(folder):\n",
    "        img_path = os.path.join(folder, filename)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        image = transform(image)\n",
    "        images.append(image)\n",
    "        filenames.append(img_path)\n",
    "    return images, filenames\n",
    "\n",
    "# Load images\n",
    "images, filenames = load_images_from_folder('data/NAB_faces_cropped/faces')\n",
    "images = torch.stack(images).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the facenet model\n",
    "\n",
    "res_model = InceptionResnetV1(pretrained=PRETRAINED_MODEL, classify=False,\n",
    "                               num_classes=None, device=device)\n",
    "facenet_embedding_net = FacenetEmbeddingNet(res_model)\n",
    "facenet_embedding_net.load_state_dict(torch.load(\"models/facenet_tune/facenet_2024_07_16_5.pth\"))\n",
    "facenet_embedding_net = facenet_embedding_net.to(device)\n",
    "\n",
    "\n",
    "# Generate embeddings for all images\n",
    "with torch.no_grad():\n",
    "    embeddings = facenet_embedding_net.get_embedding(images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cosine_similarity(embedding1, embedding2):\n",
    "    cos_sim = torch.nn.functional.cosine_similarity(embedding1, embedding2)\n",
    "    # Scale to [0, 1]\n",
    "    return (1 + cos_sim) / 2\n",
    "\n",
    "def calculate_cosine_similarity(embeddings, threshold=0.95):\n",
    "    similarity_matrix = []\n",
    "    embeddings = embeddings.cpu()\n",
    "\n",
    "    for i in range(len(embeddings)):\n",
    "        row = []\n",
    "        for j in range(len(embeddings)):\n",
    "            if i != j:\n",
    "                sim = cosine_similarity(embeddings[i].unsqueeze(0), embeddings[j].unsqueeze(0))\n",
    "                row.append(sim.item())\n",
    "            else:\n",
    "                row.append(0)\n",
    "        similarity_matrix.append(row)\n",
    "\n",
    "    groups = []\n",
    "    visited = set()\n",
    "    for i in range(len(similarity_matrix)):\n",
    "        if i not in visited:\n",
    "            group = [i]\n",
    "            visited.add(i)\n",
    "            for j in range(len(similarity_matrix[i])):\n",
    "                if similarity_matrix[i][j] > threshold and j not in visited:\n",
    "                    group.append(j)\n",
    "                    visited.add(j)\n",
    "            groups.append(group)\n",
    "    return groups\n",
    "\n",
    "\n",
    "# Calculate cosine similarity and group images\n",
    "groups = calculate_cosine_similarity(embeddings, threshold=0.95)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for CSV\n",
    "data = []\n",
    "for group_id, group in enumerate(groups):\n",
    "    person_count = group_id\n",
    "    for idx in group:\n",
    "        data.append([idx, person_count, filenames[idx]])\n",
    "\n",
    "# Create DataFrame and save to CSV\n",
    "df = pd.DataFrame(data, columns=[\"id\", \"person\", \"image path\"])\n",
    "df.to_csv(\"data/grouped_faces.csv\", index=False)\n",
    "\n",
    "print(\"CSV file created: grouped_faces.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import os\n",
    "\n",
    "# Load the CSV file containing the grouped faces information\n",
    "df = pd.read_csv(\"data/grouped_faces.csv\")\n",
    "\n",
    "# Function to show images of a specific person\n",
    "def show_images_of_person(person_id):\n",
    "    # Filter the DataFrame for the specified person\n",
    "    person_images = df[df['person'] == person_id]\n",
    "    \n",
    "    # Get the image paths\n",
    "    image_paths = person_images['image path'].tolist()\n",
    "    \n",
    "    return image_paths\n",
    "\n",
    "# Function to display the images\n",
    "# Function to display the images\n",
    "def display_images(image_paths, person_id):\n",
    "    # Set up the number of images to display\n",
    "    n_images = len(image_paths)\n",
    "    cols = 5  # Number of columns for the display\n",
    "    rows = (n_images // cols) + (n_images % cols > 0)  # Calculate the number of rows\n",
    "\n",
    "    # Create a figure to display images\n",
    "    plt.figure(figsize=(15, 3 * rows))\n",
    "    \n",
    "    for i, image_path in enumerate(image_paths):\n",
    "        # Read the image\n",
    "        img = mpimg.imread(image_path)\n",
    "        \n",
    "        # Add a subplot for each image\n",
    "        plt.subplot(rows, cols, i + 1)\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')  # Hide the axes\n",
    "        plt.title(f\"Person ID: {person_id}\")  # Set the title as the person_id\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "# Example: Get images for person 1\n",
    "person_id = 7\n",
    "image_paths = show_images_of_person(person_id)\n",
    "\n",
    "# Display images for person 1\n",
    "display_images(image_paths, person_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique person_ids from the DataFrame\n",
    "unique_person_ids = df['person'].unique()\n",
    "\n",
    "# Iterate over each unique person_id and display their images\n",
    "for person_id in unique_person_ids:\n",
    "    # Get image paths for the current person_id\n",
    "    image_paths = show_images_of_person(person_id)\n",
    "    if len(image_paths) == 1:\n",
    "        continue\n",
    "    \n",
    "    # Display images for the current person_id\n",
    "    display_images(image_paths, person_id)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "default_view": {},
   "name": "Experiments_MNIST.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
